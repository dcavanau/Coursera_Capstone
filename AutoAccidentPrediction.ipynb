{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Capstone Project - Auto Accident Prediction (Week 2)\n## Applied Data Science Capstone by IBM/Coursera\n\nThis notebook will be used for the Applied Data Science Capstone project"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of contents\n* [Introduction: Business Problem](#introduction)\n* [Data Acquisition and Cleaning](#data)\n* [Methodology](#methodology)\n* [Analysis](#analysis)\n* [Results and Discussion](#results)\n* [Conclusion](#conclusion)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Introduction: Business Problem <a name=\"introduction\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Say you are driving to another city for work or to visit some friends. It is rainy and windy. On the way to your destination, you come across a terrible traffic jam on the other side of the highway. Long lines of cars are barely moving. As you keep driving, police car start appearing from afar, shutting down the highway. There is an accident and a helicopter is transporting the ones involved in the crash to the nearest hospital. The victems must be in critical condition for all of this to be happening.\n \nNow, wouldn't it be great if there is something in place that could warn you, given the weather and the road conditions, about the possibility of you getting into a car accident and how severe it would be.  The advance warning could prompt you to  drive more carefully or even change your travel plans if you are able to."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Acquisition and Cleaning <a name=\"data\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load the required libraries"
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": "import itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport statsmodels.api as sm\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Sources\nThe data used to train and evaluate the model is the collision data set from the SDOT Traffic Management Division, Traffic Records Group. The data set is updated weekly from 2004 to the present. The data set is compiled from all collisions provided by the Seattle Police department and recorded by the Traffic Records Group.\n\n\nDownload the current collision data from <a name=Seattle Geo Data>http://data-seattlecitygis.opendata.arcgis.com</a>"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-09-15 15:01:07--  https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\nResolving opendata.arcgis.com (opendata.arcgis.com)... 54.204.141.17, 52.45.166.77, 50.19.49.12, ...\nConnecting to opendata.arcgis.com (opendata.arcgis.com)|54.204.141.17|:443... connected.\nHTTP request sent, awaiting response... 500 Internal Server Error\n2020-09-15 15:01:08 ERROR 500: Internal Server Error.\n\n"
                }
            ],
            "source": "# Live data from Seattle\n!wget -O Collisions.csv https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\n\n# Data on IBM Cloud\n#!wget -O Collisions.csv https://dataplatform.cloud.ibm.com/projects/3e2f7aff-6ac9-4bb0-aab2-69024078c07a/data-assets/57754927-cde0-43b8-8c82-caf1f02f13a6"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Load Data from CSV file\nThe data has unlabeled extra columns, which will cause an error if not accounted for. The _OBJECTID_ is used as the index for this dataset."
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] File b'/home/dsxuser/Collisions.csv' does not exist: b'/home/dsxuser/Collisions.csv'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-70-b9e835e8209f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cols = pd.read_csv('Collisions.csv', nrows=1).columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Collisions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#df = pd.read_csv('Collisions.csv', usecols=cols, index_col=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/dsxuser/Collisions.csv' does not exist: b'/home/dsxuser/Collisions.csv'"
                    ]
                }
            ],
            "source": "cols = pd.read_csv('Collisions.csv', nrows=1).columns\ndf = pd.read_csv('Collisions.csv', usecols=cols, index_col=2)\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Cleaning\n\nAn initial review of the dataset indicates that a number of features that may be safely eliminated. Thse features are used for various bookkeeping functions or are textual descriptions of categorical data. Some features such as _X_, _Y_, and _LOCATION_ are redundant. In this case, _LOCATION_ is kept."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.drop(inplace=True, columns=['X', 'Y', 'INCKEY', 'COLDETKEY', 'REPORTNO', 'STATUS', 'INTKEY', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'SEVERITYDESC', 'INCDATE', 'INCDTTM', 'SDOT_COLDESC', 'SDOTCOLNUM', 'ST_COLCODE', 'ST_COLDESC' ])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "There are problems with the dataset. There are numerous missing values that need to be filled in. \n\nThe _ADDRTYPE_, _WEATHER_, _LIGHTCOND_, _ROAD_COND_, and _JUNCTIONTYPE_ features all consist of enumerated values. There are a significant number on blank values in these fields. The blank fields were set to the value of _UNKNOWN_ in order to generate a frequency table easier.\n\nThe _INATTENTIONIND_, _UNDERINFL_, _PEDROWNOTGRNT_, _SPEEDING_, and _HITPARKEDCAR_ are binary values representing either _Yes_ or _No_. Blank values were assumed to represent a _No_ value. A _1_ is assumed to be a _Yes_ value while a _0_ is assumed to be a _No_ value. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df['ADDRTYPE'] = df['ADDRTYPE'].fillna('Unknown')\ndf[\"ADDRTYPE\"] = df[\"ADDRTYPE\"].astype('category')\ndf[\"ADDRTYPE_CAT\"] = df[\"ADDRTYPE\"].cat.codes\nprint(\"\\nAddress Type:\\n\", df['ADDRTYPE'].value_counts())\n\ndf[\"COLLISIONTYPE\"] = df[\"COLLISIONTYPE\"].astype('category')\ndf[\"COLLISIONTYPE_CAT\"] = df[\"COLLISIONTYPE\"].cat.codes\nprint(\"\\nCollision Type:\\n\", df['COLLISIONTYPE'].value_counts())\n\ndf[\"LOCATION\"] = df[\"LOCATION\"].astype('category')\ndf[\"LOCATION_CAT\"] = df[\"LOCATION\"].cat.codes\nprint(\"\\nLocation:\\n\", df['LOCATION'].value_counts())\n\ndf['WEATHER'] = df['WEATHER'].fillna('Unknown')\ndf[\"WEATHER\"] = df[\"WEATHER\"].astype('category')\ndf[\"WEATHER_CAT\"] = df[\"WEATHER\"].cat.codes\nprint(\"\\nWeather:\\n\", df['WEATHER'].value_counts())\n\ndf['LIGHTCOND'] = df['LIGHTCOND'].fillna('Unknown')\ndf[\"LIGHTCOND\"] = df[\"LIGHTCOND\"].astype('category')\ndf[\"LIGHTCOND_CAT\"] = df[\"LIGHTCOND\"].cat.codes\nprint(\"\\nLight Conditions:\\n\", df['LIGHTCOND'].value_counts())\n\ndf['ROADCOND'] = df['ROADCOND'].fillna('Unknown')\ndf[\"ROADCOND\"] = df[\"ROADCOND\"].astype('category')\ndf[\"ROADCOND_CAT\"] = df[\"ROADCOND\"].cat.codes\nprint(\"\\nRoad Conditions:\\n\", df['ROADCOND'].value_counts())\n\ndf['JUNCTIONTYPE'] = df['JUNCTIONTYPE'].fillna('Unknown')\ndf[\"JUNCTIONTYPE\"] = df[\"JUNCTIONTYPE\"].astype('category')\ndf[\"JUNCTIONTYPE_CAT\"] = df[\"JUNCTIONTYPE\"].cat.codes\nprint(\"\\nJunction Type:\\n\", df['JUNCTIONTYPE'].value_counts())\n\n# treat a blank record as 0, an N as 0 and Y as 1\ndf['INATTENTIONIND'] = df['INATTENTIONIND'].fillna('0')\ndf['INATTENTIONIND'] = df['INATTENTIONIND'].replace(['N','Y'],['0','1'])\ndf[\"INATTENTIONIND\"] = df[\"INATTENTIONIND\"].astype('int64')\nprint(\"\\nInattention Indicator:\\n\", df['INATTENTIONIND'].value_counts())\n\n# treat a blank record as 0, an N as 0 and Y as 1\ndf['UNDERINFL'] = df['UNDERINFL'].fillna('0')\ndf['UNDERINFL'] = df['UNDERINFL'].replace(['N','Y'],['0','1'])\ndf[\"UNDERINFL\"] = df[\"UNDERINFL\"].astype('int64')\nprint(\"\\nUnder Influence:\\n\", df['UNDERINFL'].value_counts())\n\n# treat a blank record as 0, an N as 0 and Y as 1\ndf['PEDROWNOTGRNT'] = df['PEDROWNOTGRNT'].fillna('0')\ndf['PEDROWNOTGRNT'] = df['PEDROWNOTGRNT'].replace(['N','Y'],['0','1'])\ndf[\"PEDROWNOTGRNT\"] = df[\"PEDROWNOTGRNT\"].astype('int64')\nprint(\"\\nPedestrian Not Granted:\\n\", df['PEDROWNOTGRNT'].value_counts())\n\n# treat a blank record as 0, an N as 0 and Y as 1\ndf['SPEEDING'] = df['SPEEDING'].fillna('0')\ndf['SPEEDING'] = df['SPEEDING'].replace(['N','Y'],['0','1'])\ndf[\"SPEEDING\"] = df[\"SPEEDING\"].astype('int64')\nprint(\"\\nSpeeding:\\n\", df['SPEEDING'].value_counts())\n\n# treat a blank record as 0, an N as 0 and Y as 1\ndf['HITPARKEDCAR'] = df['HITPARKEDCAR'].fillna('0')\ndf['HITPARKEDCAR'] = df['HITPARKEDCAR'].replace(['N','Y'],['0','1'])\ndf[\"HITPARKEDCAR\"] = df[\"HITPARKEDCAR\"].astype('int64')\nprint(\"\\nHit Parked Car:\\n\", df['HITPARKEDCAR'].value_counts())\n\nprint(\"\\nPerson Count:\\n\", df['PERSONCOUNT'].value_counts())\n\nprint(\"\\nVehicle Count:\\n\", df['VEHCOUNT'].value_counts())\n\ndf[\"SEVERITYCODE\"] = df[\"SEVERITYCODE\"].astype('category')\ndf[\"SEVERITYCODE_CAT\"] = df[\"SEVERITYCODE\"].cat.codes\n\ndf['SDOT_COLCODE'] = df['SDOT_COLCODE'].fillna('0')\ndf[\"SDOT_COLCODE\"] = df[\"SDOT_COLCODE\"].astype('int64')\nprint(\"\\nState DOT Collision Code:\\n\", df['SDOT_COLCODE'].value_counts())\n\n\ndf['SDOT_COLCODE'] = df['SDOT_COLCODE'].fillna('0')\ndf[\"SDOT_COLCODE\"] = df[\"SDOT_COLCODE\"].astype('int64')\nprint(\"\\nState DOT Collision Code:\\n\", df['SDOT_COLCODE'].value_counts())\n\n\n\nprint(\"\\nSeverity Code:\\n\", df['SEVERITYCODE'].value_counts())\n\n#df.shape\ndf.dtypes"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature Selection\n\nAfter data cleaning, there were 221,389 samples with 26 features. There are outliers in the data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The _WEATHER_ feather has several outliers. I changed the _WEATHER_ categories of _Snowing_, _Fog/Smog/Snow_, _Sleet/Hail/Freezing_Rain_, _Blowing Sand/Dirt_, _Severe Crosswind_, _Partley Cloudy_, and _Blowing Snow_ to _Other_. These categories are not major factors in the data set and can be safely combined. Timestamps are not available. If Timestamps were available, the _Unknown_ values could be set to reflect the appropriate weather conditions.\n\nThe frequency tables are then regenerated to determine if _Other_ and _Unknown_ are significant."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['WEATHER'].replace({'Snowing':'Other', 'Fog/Smog/Smoke':'Other', 'Sleet/Hail/Freezing Rain':'Other', 'Blowing Sand/Dirt':'Other', 'Severe Crosswind':'Other', 'Partly Cloudy':'Other', 'Blowing Snow':'Other'}, value=None, inplace=True)\nprint(\"\\nWeather:\\n\", df['WEATHER'].value_counts())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The _LIGHTCOND_ feather has several outliers. I changed the _LIGHTCOND_ categories of _Dark - Street Lights On_, _Dark - No Street Lights_, _Dark - Street Lights Off_, and _Dark - Unknown Lighting_ to _Dark_. These categories are not major factors in the data set and can be safely combined. Timestamps are not available. If Timestamps were available, the _Unknown_ values could be set to reflect the appropriate light conditions.\n\nThe frequency table was then regenerated."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['LIGHTCOND'].replace({'Dark - Street Lights On':'Dark', 'Dark - No Street Lights':'Dark', 'Dark - Street Lights Off':'Dark', 'Dark - Unknown Lighting':'Dark'}, value=None, inplace=True)\nprint(\"\\nLight Conditions:\\n\", df['LIGHTCOND'].value_counts())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The _ROADCOND_ feature has several outliers. I changed the _ROADCOND_ categories of _Ice_, _Snow/Slush_, _Standing Water_, _Sand/Mud/Dirt_, and _Oil_ to _Other_. These categories are not major factors in the data set and can be safely combined.\n\nThe frequency table was then regenerated to determine if _Other_ is now significant."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df['ROADCOND'].replace({'Ice':'Other', 'Snow/Slush':'Other', 'Standing Water':'Other', 'Sand/Mud/Dirt':'Other', 'Oil':'Other'}, value=None, inplace=True)\nprint(\"\\nRoad Conditions:\\n\", df['ROADCOND'].value_counts())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "A heatmap is generated to examinine the correlation of independent variables."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.figure(figsize=(20, 20))\ncor = df.corr()\nsns.heatmap(cor, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=3, linecolor='black')\nplt.show()\n\n#Correlation with output variable\ncor_target = abs(cor[\"SEVERITYCODE_CAT\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.25]\nrelevant_features"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Backward Elimination\nX = df.drop(columns=['SEVERITYCODE_CAT', 'SEVERITYCODE','ADDRTYPE', 'COLLISIONTYPE', 'LOCATION', 'WEATHER', 'LIGHTCOND', 'ROADCOND', 'JUNCTIONTYPE'])   #Feature Matrix\ny = df['SEVERITYCODE_CAT'] \ncols = list(X.columns)\n\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)\n\n\nreg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"Rows before cleaning: \", df.shape)\ndf = df[~df['WEATHER'].isin(['Unknown'])]\ndf = df[~df['WEATHER'].isin(['Other'])]\ndf = df[~df['LIGHTCOND'].isin(['Unknown'])]\ndf = df[~df['LIGHTCOND'].isin(['Other'])]\ndf = df[~df['ROADCOND'].isin(['Unknown'])]\ndf = df[~df['ROADCOND'].isin(['Other'])]\ndf = df[~df['SEVERITYDESC'].isin(['Unknown'])]\nprint (\"Rows after cleaning: \", df.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Convert Categorical Features to Numeric Values\n\nThe feature set for the model consists of the following:\n* PERSONCOUNT\n* VEHCOUNT\n* WEATHER\n* ROADCOND\n* LIGHTCOND\n\nOne Hot Encoding will be used for the latter three features."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Feature = df[['PERSONCOUNT','VEHCOUNT']]\n# do One Hot Encoding\nFeature = pd.concat([Feature,pd.get_dummies(df['WEATHER'])], axis=1)\nFeature = pd.concat([Feature,pd.get_dummies(df['ROADCOND'])], axis=1)\nFeature = pd.concat([Feature,pd.get_dummies(df['LIGHTCOND'])], axis=1)\nFeature.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The desired prediction value will be _SEVERITY_DESC_. The feature dataset will be run through the Standard Scaller to normalize all of the data. The dataset will then be split into a training set for the models and a test set to evaluate how good the models are at prediction."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X = Feature\ny = df['SEVERITYDESC'].values\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Methodology <a name=\"methodology\"></a>\nIn this project we will direct our efforts on predicing accidents based on the feature set defined above."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Analysis <a name=\"analysis\"></a>\nFour models will be developed. The accurracy, Jac"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## K Nearest Neighbor (KNN)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "Ks = 10\nmean_accKNN = np.zeros((Ks-1))\nmean_jacKNN = np.zeros((Ks-1))\nmean_F1KNN  = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict\n    print (\"Ks = \", n)\n    neigh = KNeighborsClassifier(n_neighbors=n, n_jobs=1, weights='distance').fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n\n    mean_accKNN[n-1] = metrics.accuracy_score(y_test, yhat)\n    mean_jacKNN[n-1] = jaccard_similarity_score(y_test, yhat)\n    mean_F1KNN[n-1]  = f1_score(y_test, yhat, average='weighted')\n    \nprint (\"KNN Accuracy table: \", mean_accKNN)\nprint( \"The best accuracy is\", mean_accKNN.max(), \"with k=\", mean_accKNN.argmax()+1) \n\nprint (\"KNN Jaccard index table: \", mean_jacKNN)\nprint( \"The best Jaccard index is\", mean_jacKNN.max(), \"with k=\", mean_jacKNN.argmax()+1) \n\nprint (\"KNN F1-score table: \", mean_F1KNN)\nprint( \"The best F1-score is\", mean_F1KNN.max(), \"with k=\", mean_F1KNN.argmax()+1) "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Decision Tree"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.tree import DecisionTreeClassifier\nDT_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 10)\nDT_model.fit(X_train,y_train)\nDT_model\n\nKs = 10\nmean_accDT = np.zeros((Ks-1))\nmean_jacDT = np.zeros((Ks-1))\nmean_F1DT  = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict\n    print (\"Ks = \", n)\n    DT_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = n)\n    DT_model.fit(X_train,y_train)\n    yhat = DT_model.predict(X_test)\n\n    mean_accDT[n-1] = metrics.accuracy_score(y_test, yhat)\n    mean_jacDT[n-1] = jaccard_similarity_score(y_test, yhat)\n    mean_F1DT[n-1]  = f1_score(y_test, yhat, average='weighted')\n    \nprint (\"Decision Tree Accuracy table: \", mean_accDT)\nprint( \"The best accuracy is\", mean_accDT.max(), \"with k=\", mean_accDT.argmax()+1) \n\nprint (\"Decision Tree Jaccard index table: \", mean_jacDT)\nprint( \"The best Jaccard index is\", mean_jacDT.max(), \"with k=\", mean_jacDT.argmax()+1) \n\nprint (\"Decision Tree F1-score table: \", mean_F1DT)\nprint( \"The best F1-score is\", mean_F1DT.max(), \"with k=\", mean_F1DT.argmax()+1) "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Support Vector Machine\nUse the LinearSVC because of the large number of samples."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn import svm\nSVM_model = svm.LinearSVC()\nSVM_model.fit(X_train, y_train) "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat = SVM_model.predict(X_test)\nprint (\"The SVM model accuracy is: \", metrics.accuracy_score(y_test, yhat))\nprint(\"SVM Jaccard index: %.2f\" % jaccard_similarity_score(y_test, yhat))\nprint(\"SVM F1-score: %.2f\" % f1_score(y_test, yhat, average='weighted') )\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Logistic Regression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.linear_model import LogisticRegression\nLR_model = LogisticRegression(C=0.01).fit(X_train,y_train)\nLR_model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "yhat = LR_model.predict(X_test)\nprint (\"The LR model accuracy is: \", metrics.accuracy_score(y_test, yhat))\nyhat_prob = LR_model.predict_proba(X_test)\nprint(\"LR Jaccard index: %.2f\" % jaccard_similarity_score(y_test, yhat))\nprint(\"LR F1-score: %.2f\" % f1_score(y_test, yhat, average='weighted') )\nprint(\"LR LogLoss: %.2f\" % log_loss(y_test, yhat_prob))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Results and Discussion <a name=\"results\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Conclusion <a name=\"conclusion\"></a>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}